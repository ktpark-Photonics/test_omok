# Omok Reinforcement Learning Trainer

이 저장소는 스트림릿(Streamlit) 기반의 UI를 제공하여 두 개의 에이전트가 자기대국(Self-Play)을 통해 오목(Omok, Gomoku) 전략을 학습하고, 학습된 모델과 직접 대국할 수 있도록 돕습니다. 실시간 학습 지표 확인, 체크포인트 저장/불러오기 기능을 지원해 누구나 간편하게 강화학습 워크플로우를 체험할 수 있습니다.

## 주요 기능

- **자기대국 학습**: 두 개의 정책 신경망이 서로 대결하면서 승패 보상으로 정책을 업데이트합니다.
- **실시간 학습 모니터링**: 이동 평균 승률과 손실(loss)을 차트 및 지표로 확인할 수 있습니다.
- **체크포인트 관리**: 학습된 가중치를 손쉽게 저장하거나 클릭 한 번으로 다시 불러올 수 있습니다.
- **직접 대국**: 학습된 에이전트와 브라우저에서 바로 오목을 둘 수 있습니다.

## 설치 및 실행 방법

1. Python 3.9 이상 환경을 준비합니다.
2. 의존성을 설치합니다.

   ```bash
   pip install -r requirements.txt
   ```

3. 스트림릿 앱을 실행합니다.

   ```bash
   streamlit run app.py
   ```

4. 브라우저가 자동으로 열리지 않는 경우, 터미널에 표시되는 로컬 URL (예: `http://localhost:8501`) 을 브라우저에서 열어 접속합니다.

## 사용 방법

### 1. 학습 탭

- **바둑판 크기**, **에피소드 수**, **학습률**, **탐험 비율** 등을 조정한 뒤 `학습 시작` 버튼을 누르면 자기대국 학습이 진행됩니다.
- 학습 중에는 승률/손실 지표가 실시간으로 갱신되고, 진행률도 확인할 수 있습니다.

### 2. 플레이 탭

- 학습이 끝난 뒤 또는 체크포인트를 불러온 후, `플레이` 탭에서 에이전트와 직접 오목을 둘 수 있습니다.
- 내 돌 색(흑/백)을 선택하고 `새 게임 시작`을 누르면 대국이 시작됩니다.
- 보드의 빈 칸을 클릭하여 수를 두면 에이전트가 즉시 응수합니다.

### 3. 체크포인트 탭

- 현재 메모리에 올라와 있는 에이전트를 원하는 이름으로 저장할 수 있습니다.
- 저장된 체크포인트 목록에서 `이 체크포인트 불러오기` 버튼을 누르면 해당 상태가 즉시 복원됩니다.

## 프로젝트 구조

```
.
├── app.py                # Streamlit 애플리케이션 엔트리 포인트
├── omok/
│   ├── __init__.py
│   ├── agent.py          # 정책 신경망 및 학습 에이전트
│   ├── game.py           # 오목 게임 규칙 및 상태 관리
│   ├── training.py       # 자기대국 학습 루프
│   └── checkpoint.py     # 체크포인트 저장/불러오기 유틸리티
├── requirements.txt
└── README.md
```

## 참고 사항

- 기본 학습 알고리즘은 정책 그라디언트(REINFORCE) 기반으로 구현되어 있으며, 단순한 예제용 구조입니다. 하이퍼파라미터를 변경하거나 네트워크 구조를 확장하여 성능을 향상시킬 수 있습니다.
- GPU가 있다면 자동으로 활용되며, 없는 경우 CPU에서 동작합니다.
- 체크포인트 파일은 프로젝트 루트의 `checkpoints/` 폴더에 저장됩니다.

즐거운 실험 되세요! 🎮
